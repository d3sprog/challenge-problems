\documentclass[english,submission]{programming}

\usepackage{inputenx}
\usepackage[backend=bibtex]{biblatex}
\addbibresource{paper.bib}

\usepackage{csquotes}
\usepackage{changepage}
\usepackage{multicol}
\usepackage{ccicons}
\usepackage[many]{tcolorbox}
\usepackage{tikz-cd}
\usepackage{float}

\lstdefinelanguage[programming]{TeX}[AlLaTeX]{TeX}{%
  deletetexcs={title,author,bibliography},%
  deletekeywords={tabular},
  morekeywords={abstract},%
  moretexcs={chapter},%
  moretexcs=[2]{title,author,subtitle,keywords,maketitle,titlerunning,authorinfo,affiliation,authorrunning,paperdetails,acks,email},
  moretexcs=[3]{addbibresource,printbibliography,bibliography},%
}%
\lstset{%
  language={[programming]TeX},%
  keywordstyle=\firamedium,
  belowskip=1em,
  aboveskip=1em,
  framesep=0.5em,
  basicstyle=\normalsize\ttfamily,
  %stringstyle=\color{RosyBrown},%
  %texcsstyle=*{\color{Purple}\mdseries},%
  %texcsstyle=*[2]{\color{Blue1}},%
  %texcsstyle=*[3]{\color{ForestGreen}},%
  %commentstyle={\color{FireBrick}},%
  numbersep=1em,
  xleftmargin=1.5em,
  escapechar=`,}
\newcommand*{\CTAN}[1]{\href{http://ctan.org/tex-archive/#1}{\nolinkurl{CTAN:#1}}}
%%

\newcounter{challengen}
\newcommand{\challenge}[1]{\subsection*{\addtocounter{challengen}{1}Challenge \#\thechallengen: #1}}

\DeclareRobustCommand{\frameworkbox}[2][gray!15]{
\begin{tcolorbox}[breakable,left=3pt,right=3pt,top=3pt,bottom=3pt,colback=#1,colframe=#1,parbox=false,
  width=\dimexpr\textwidth\relax,enlarge left by=0mm,boxsep=5pt,arc=0pt,enlarge top by=0.5em,%enlarge bottom by=0.0em,
  outer arc=0pt]\setlength{\parskip}{0.5em}\setlength{\parindent}{0em}{\firamedium Framework perspective.}\quad #2
\end{tcolorbox}}

\paperdetails{
  perspective=engineering,
  area={Programming Systems},
}

\begin{document}

% define natbib citet
\newcommand{\citet}[1]{\citeauthor*{#1}~\cite{#1}}


\title{Type Evolution in Programming Systems}

\author[a]{Jonathan Edwards}%[0000-0003-1958-7967]
\authorinfo{is TBD. Contact him at \email{jonathanmedwards@gmail.com}.}
\affiliation[a]{Independent, Boston, MA, USA}

\author[b]{Tomas Petricek}%[0000-0002-7242-2208]
\authorinfo{is TBD. Contact him at \email{tomas@tomasp.net}.}
\affiliation[b]{Charles University, Prague, Czechia}

\author[c,d]{Tijs van der Storm}%[0000-0001-8853-7934]
\authorinfo{is TBD. Contact him at \email{storm@cwi.nl}.}
\affiliation[c]{Centrum Wiskunde \&\ Informatica (CWI), Amsterdam, Netherlands}
\affiliation[d]{University of Groningen, Groningen, Netherlands}

\author[e]{Geoffrey Litt}%[0000-0003-0858-5165]
\authorinfo{is TBD. Contact him at \email{gklitt@gmail.com}.}
\affiliation[e]{Ink \& Switch, Planet Earth}

\authorrunning{J. Edwards, T. Petricek, T. van der Storm, G. Litt}

\keywords{Type Evolution, Schema Evolution, Live Programming, Local-first Programming}

% TODO: Please go to https://dl.acm.org/ccs/ccs.cfm and generate your Classification System

\maketitle

\begin{abstract}

% Context: What is the broad context of the work? What is the importance of the general research area?
Many recent programming improvements have been enabled by shorter feedback loops. To further
advance the state of programming, we need programming systems that enable rapid collaboration
and make programming more live across the entire development stack. To do this, programming
systems need to preserve state during development and interaction with the programmer.

% Inquiry: What problem or question does the paper address? How has this problem or question been addressed?
The key obstacle to live collaborative programming systems is the problem of \emph{type evolution}.
When programmer changes the structure of a type, programming system needs to co-evolve
existing live data and adapt code that implements the program behaviour to match the new structure.
This problem has been studied in isolation as schema evolution in databases, hot code reloading
in live programming and dynamic software updating, but we lack a unified comprehensive perspective.

% Approach: What was done that unveiled new knowledge?
In this paper, we develop a unfied conceptual framework for analysing the problem of type evolution.
We use the framework to study type evolution in five diverse case studies.
%
% Knowledge: What new facts were uncovered? What new capabilities are enabled by the work?
By looking at the case studies through a unified perspective, we hope to inspire new thinking
about the problem of type evolution and also encourage knowledge transfer across sub-fields.

% Grounding: What argument, feasibility proof, artifacts, or results and evaluation support this work?
Our work takes the form of a sequence of challenge problems drawn from real-world programming
scenario in diverse field such as live programming, database programming and model-driven
development.
%
% Importance: Why does this work matter?
Tackling the problem of type evolution in a unified way, enabled by our new conceptual framework,
will make it possible to design and build a future generation of effective, live and collaborative
programming systems.

\end{abstract}

% ==================================================================================================
% INTRODUCTION
% ==================================================================================================

\section{Introduction}

Many past improvements in the practice of programming worked by speeding up feedback loops,
both in collaboration with other programmers (through collaborative development tools
\cite{goldman2011real,kurniawan2015coder,replit}) and in individual's interaction with the
programming environment (through live programming \cite{rein2018exploratory}).

To make programming more effective, future programming systems will need to further tighten
the feedback loop. They will need to enable programmers to collaborate at a fine-grained level,
letting them adopt individual changes created by their collaborators (as in local-first software
\cite{localfirst}). To provide live feedback, future programming systems will also need to
transcend treating state as ephemeral and reset when the
program is edited and rerun.

To conceive of such systems, we need to shift our focus to \emph{programming systems} \cite{techdims}
where program state is persistent and co-evolves with the behavior and data of the system. This is
already the case in collaborative applications with state persistent in a database, image-based
systems like Smalltalk and live programming environments with hot-reloading where the program can
be edited without resetting all state.

While we are firmly convinced that more collaborative and live programming systems are the
future of programming, there are a number of problems that need to be resolved in order for
such systems to become a reality. A key problem is \emph{type evolution}, i.e., how to
gracefully adapt the shape of data used in a program while preserving integrity
constraints, existing data and the logic of code.

Type evolution is not a new problem. It exists in various forms in a range of existing systems.
The problem of schema evolution in databases is well-studied \cite{erhard06}.
It typically considers data integrity, although co-evolution of database schema
and code has also been studied \cite{Cleve2006,wang19}.
In live programming, the problem is known as hot code reloading \cite{barenz2020essence,beckmann2021shortening},
although it typically focuses on updating transient state and ignores the problem of more complex
data migrations. The problem is also studied in the bidirectional transformation (BX)
community \cite{czarnecki2009bidirectional}, in work on model-driven development
\cite{Cicchetti11,alanen2003} and in more low-level work on dynamic software updating
\cite{hicks2005dynamic}.

Looking at type evolution from the perspective of programming systems can offer a new perspective that
unfies the many different aspects of the problem partially explored in other areas. A programming
system may have permanent data (as in a database) whose structure can change, requiring
corresponding change to data and code. A programming system may have a transient state conforming
to a type structure that may change during live programming. A programming system may also need
to allow programmers to independently work on different versions of a program.

In this paper, we draw connections between these different kinds of type evolution, pointing out
an essential shared structure. First, we present a shared conceptual
framework for thinking about the relationship between schema, code, and data in a programming
system. Second, we present five case studies that use the framework to study type evolution in a
highly diverse set of real-world programming scenarios. We hope this will encourage knowledge
sharing across different subfields, motivate further research on this important problem and
offer inspiring challenges that developers of future live and collaborative programming systems
will attempt to tackle.

\begin{figure}[t]
\centering
\vspace{-1em}
\includegraphics[width=0.45\textwidth]{figures/layers.png}
\caption{Pace layers from Stewart Brand's \emph{How Buildings Learn} \cite{Brand95}.
  Different layers of a building change at different pace. The geographic
  site of a building is eternal, its structure remains stable for decades, space plan
  changes every couple of years.}
\label{fig:layers}
\end{figure}

% ==================================================================================================
% FRAMEWORK
% ==================================================================================================

\section{Type Evolution}
To paraphrase Stewart Brand \cite{Brand95}, ``almost no programs evolve well. But all programs
(except for monuments) evolve anyway, however poorly, because the usages in and around them are
changing constantly.'' Programs evolve both while they are being created and during their operation
in response to new needs. As with buildings, different aspects of program evolve at different pace
(Figure~\ref{fig:layers}). And ``because of the different rates of change of its components,
a program is always tearing itself apart.''

\subsection{Types, Data and Code}

As is the case for buildings, making a change at surface layers (skin or stuff) is relatively easy,
but a change at a more fundamental layer (structure) has cascading effects on layers that depend on it.
In the case of programs, a more fundamental layer defines the shape of data and code.
We refer to this structure as \emph{types}, although we do not use the term in a precise technical
sense. In our conceptual framework, types may be data types that define the shape of data, they
may be explicitly declared in code and checked statically, as well as dynamically checked or
even implicit. The key property of types for us is that they constrain the shape of other layers
of a program, namely data and code. As a result, a change of program types requires a
corresponding change in program data and code.

The situation is illustrated in Figure~\ref{fig:change} using two variations of a diagram that
we will return to repeatedly in this paper. When a type changes from $T_1$ to $T_2$,
we need to find and apply a corresponding change to other parts of the program whose shape
is defined by the changed type. The problem of \emph{type evolution} is thus finding and
applying changes that will update data $D_1$ and code $C_1$ into new versions $D_2$ and $C_2$
that correspond to the new type.

The complexity of the problem varies. The first dimension of challenges that we consider
is local. The complexity of the challenge depends on how many other parts of the program
depend on the modified type. If both code and data depend on the type, the complexity of the
program is greater. If there is no associated code or the existing data can be safely discarded,
the complexity is lesser.

The second dimension of challenges that we consider is non-local. A program may have multiple
variants that exist independently and need to be synchronized. The difficulty of the problem
depends on what kind of synchronization we want to support. If all variants of the program should
eventually adopt all changes (converge), the complexity is lesser. If we allow user to apply
only certain changes from other users (divergence), the complexity is greater. A typical case
may be a combination of the two where types and schema of a program should converge, but data
may diverge.

\begin{figure}[t]
\centering
\includegraphics[width=0.6\textwidth]{figures/arrows.png}
\caption{When a type changes from $T_1$ to $T_2$, a corresponding change needs to take place
  in data and code whose shape is defined by type. The challenge is finding a suitable
  corresponding transformation (semi-)automatically.}
\label{fig:change}
\end{figure}

\subsection{Conceptual Framework}

We now describe a conceptual framework that lets us view a diverse set of real-world
type evolution problems in a unified way. We distinguish between two dimensions.
The \emph{local dimension} is concerned with how type evolution affects data and code within
a single program. The \emph{non-local dimension} is concerned with how type evolution
propagates across different program variants that may exist independently.

\paragraph{Program Layers.}
We consider the more permanent layer of \emph{types} and two less permanent layers of code and
data that are structured by types. In specific case studies, the layers may map to
different aspects of programs, some may be not present and some may be implicit.

\begin{itemize}
\item \emph{Types} -- structure program data and determine aspects of code. A change in types
  requires a corresponding change in data and code. Types may be explicit (database schema, type definition)
  or implicit (shape of data structures, shape of a document).
\item \emph{Data} -- information stored by the program. Data may be more permanent (data in a
  database) or less permanent (state in live programming). When types evolve, data needs to be
  updated or (in case of transient state) discarded.
\item \emph{Code} -- or program logic, but not including source code that defines types.
  When types evolve, code needs to change to consume/produce data of a correct shape.
\end{itemize}

\paragraph{Program Variants.}
In programming systems that enable collaboration, multiple variants of a program may exist
concurrently and their types, data and code may need to be synchronized. Although collaboration
is missing from some of our case studies, it adds an important dimension that is crucial for
type evolution in future programming systems. We consider two characteristics.

\begin{itemize}
\item \emph{Convergence} vs. \emph{Divergence}. In the convergence model, all program variants
  eventually adopt all changes. It may not be possible to adopt a change without adopting
  all earlier changes. In the divergence model, a user may choose only
  particular changes they want to adopt, but keep other aspects of their custom design.
\item \emph{Centralized} vs. \emph{Decentralized}. In the centralized model, changes (type
  evolution) can only originate from a particular source. In the decentralized model,
  changes can be done on any of the multiple co-existing variants of a program.
\end{itemize}

\paragraph{Challenge Problems.}
The conceptual framework offers a wide range of configurations for challenge problems, but
our aim is not to enumerate all options. We use it to recast a number of
existing challenges in real-world programming systems as instances of the same problem of
type evolution. The case studies show how well-known solutions or problems in one domain
map to another domain and they also provide and inspiration and a benchmark for
developers of future programming systems.

% ==================================================================================================
% ELM
% ==================================================================================================

\section{Live Programming for the Elm Architecture}
\label{sec:elm}

The first challenge problem that we present in terms of our conceptual framework is live programming
of user interfaces based on the Elm architecture. In this model, a reactive web application is
structured in terms of current state and events that affect the state. The implementation then
consists of two functions called \texttt{update} and \texttt{render}:

\begin{lstlisting}[language=ml,morekeywords={on}]
type State = { .. }
type Event = .. | ..

val update : State -> Event -> State
val render : State -> Html
\end{lstlisting}

\noindent
The programming model works as follows:

\begin{itemize}
\setlength\itemsep{0em}
\item \texttt{State} represents the application state, i.e., everything that the user can work with.
\item \texttt{Event} represents events that the user can trigger by interacting with the application.
\item \texttt{update} is called when an event happens and computes a new program state.
\item \texttt{render} takes the current state and produces a visual representation of the page.
\end{itemize}

\noindent
As an example, consider the well-known TODO list app. Its state consists of a list of items,
each with a unique ID, a title and a flag indicating whether it has been completed.
The events represent changing of an item, deletion and addition:

\begin{lstlisting}[language=ml]
type Item = { id : id; title : string, completed : bool }
type State = { items : Item list }
type Event =
  | SetTitle of id * string
  | SetCompleted of id * bool
  | Remove of id
  | Add of string
\end{lstlisting}

\noindent
Some programming systems that use the Elm architecture support hot reloading when the
implementation of the \texttt{render} or \texttt{update} function changes, but they typically
discard state and restart the application when the \texttt{State} type changes. A more effective
programming system based on live programming would allow updates to both the code of the two
functions and the structure of the two types.

\begin{table}[t]
\begin{tabular}{lll}\toprule
{\firamedium Change} & {\firamedium What} & {\firamedium How}\\\midrule
Add & case to \texttt{Event} & Requires adding corresponding case to \texttt{update} \\
Remove & case from \texttt{Event} & Remove unused code from \texttt{update} \\
Add & field to \texttt{State} & Migrate state value and initialize field \\
Remove & field from \texttt{State} & Migrate state (assuming field unused) \\
Modify & structure of \texttt{State} & Migrate state value and edit code accordingly \\
\bottomrule
\end{tabular}
\vspace{0.3em}
\caption{Changes to types and how a programming system should handle them}
\label{tbl:elmchanges}
\vspace{-1em}
\end{table}

\challenge{Live State Type Evolution}
Assume that we have a running TODO list with the above state and events. To test the application,
the programmer has already created a number of items and so there is a value of the
\texttt{State} type that represents the current state of the application such as:

\begin{lstlisting}[language=ml,morekeywords={true,false}]
{ items : [
   { id = 1; title = "Check Twitter"; completed = true }
   { id = 2; title = "Write the paper"; completed = false } ] }
\end{lstlisting}

\noindent
There is a number of edits to the types that the programmer may now want to do without
restarting the application and losing the data. For example, they may want to change the
\texttt{completed} field of \texttt{Item} to instead store optional completion time:

\begin{lstlisting}[language=ml]
type Item = { id : id; title : string; completed : DateTime option }
type State = { items : Item list }
type Event =
  | .. | SetCompleted of id * DateTime option | ..
\end{lstlisting}

\noindent
To apply the change without restarting the application, the existing data need to be migrated
to match the new type definition. In this case, this likely cannot be done fully automatically
and the programmer may need to provide mapping (\texttt{false} to \texttt{None}, \texttt{true}
to \texttt{Some(DateTime(2024,5,1))}). If the programmer changes \texttt{Item} and \texttt{SetCompleted}
at the same time, the \texttt{update} function likely does not need to change, but the rendering
code needs to be modified (the programmer needs to specify how to map \texttt{DateTime option}
back to a Boolean for a checkbox).

More generally, there is a number of edits to the types that the programmer may want to do
without restarting the application. The edits and the desired handling in the programming
system are summarized in Table~\ref{tbl:elmchanges}. Modifying the \texttt{Event} type does not
require data migration (assuming that the change does not happen when there are unprocessed
events). It may result in unused code or missing cases in \texttt{update} that need to be addressed.
Modifying \texttt{State} ranges from relatively simple problems (e.g., adding a new field with a
default value) to challenging case. In particular, if the programer refactors \texttt{State} to a
semantically equivalent type, the programming system should, in principle, be able to automatically
migrate both the original value and implementation of both functions to match the new type.

\frameworkbox{
The evolving \emph{type} in this case is the \texttt{State} type, \emph{code} is the
program logic implemented in \texttt{update} and \texttt{render} and \emph{data} is the current
state. To support live programming, a local type change requires synchronizing local data to
match the new type, and also a change in code so that it continues working against the new type
structure. In a non-local scenario, programmers would likely want to synchronize types and code,
but not necessarily data. A collaborative programming system would need to support decentralized
and possibly divergent type evolution, but without data synchronization.
}

\subsection{Remarks}
The key requirement from the live programming perspective is to ``do minimal harm''.
The migration of \emph{data} needs to be done automatically and the system should strive
to produce a new state that is as near as possible to the previous state. Migrating
\emph{code} can be done in various ways (transform code, trigger error condition, etc.),
but it is desirable to avoid breaking the \texttt{render} function as this would make the
new application state impossible to visualize.

The specific case where the type is refactored to a semantically equivalent type is related to
the Extract Entity challenge discussed below. Finally, a programming system tackling this
challenge may rely on structure editing so that the system has access to a high-level logical
description of the edits performed by the user.

% ==================================================================================================
% DATABASES
% ==================================================================================================

\section{Entity Evolution in Data-Centric Systems}
Many problems studied in the context of database systems equally apply to programming systems
if we think of them as stateful environments where programs co-exist with their persistent data.
Although schema evolution is well-studied in databses (Section~\ref{sec:related}), past work
does not resolve all problems in our second challenge.

As an example, consider the Acme Corporation that records orders from various customers for
various products. They use a spreadsheet (Figure~\ref{fig:db-orders}) with a row for each order
and columns for information about the customer and product. The shipping department filters this
table on blank ship dates to see what they need to ship. But after a while the orders department
realizes they are wasting effort duplicating the address for new order from an old customer. And
when the customer's address changes they have to go back and edit all of their orders.

\begin{figure}
\begin{adjustwidth}{-2em}{-3em}
\begin{subfigure}[b]{35em}\vspace{0pt}
  \sffamily
  \small
  \begin{tabular}{ |r|l|r|r|l|l|}
     \hline
     oid & item & quantity & ship\_date & customer\_name & customer\_address \\
     \hline \hline
     1 & Anvil & 1 & 2/3/23 & Wile E Coyote & 123 Desert Station \\
     \hline
     2 & Dynamite & 2 & & Daffy Duck & White Rock Lake \\
     \hline
     3 & Bird Seed & 1 & & Wile E Coyote & 123 Desert Station \\
     \hline
  \end{tabular}
  \caption{Spreadsheet recording orders with all associated information}
  \label{fig:db-orders}
\end{subfigure}
\hfill
\\[-0.5em]~\\
\begin{subfigure}[b]{20em}\vspace{0pt}
  \sffamily
  \small
  \begin{tabular}{ |r|l|l|}
    \hline
    cid & customer\_name & customer\_address \\
    \hline \hline
    1 & Wile E Coyote & 123 Desert Station \\
    \hline
    2 & Daffy Duck & White Rock Lake \\
    \hline
  \end{tabular}
  \caption{Customers table after type evolution}
  \label{fig:db-customers}
\end{subfigure}
\hfill
\begin{subfigure}[b]{20em}\vspace{0pt}
  \sffamily
  \small
  \begin{tabular}{ |r|l|r|r|r|}
     \hline
     oid & item & quantity & ship\_date & cid \\
     \hline \hline
     1 & Anvil & 1 & 2/3/23 & 1 \\
     \hline
     2 & Dynamite & 2 & & 2  \\
     \hline
     3 & Bird Seed & 1 & & 1 \\
     \hline
    \end{tabular}
  \caption{Orders linking to Customers, after type evolution}
 \label{fig:db-orders-link}
\end{subfigure}
\end{adjustwidth}
\vspace{0.25em}
\caption{Types and data before and after performing Extract Entity}
\label{fig:db}
\end{figure}

\challenge{Extract Entity}
Acme needs to evolve the above type and data into two tables (Figure~\ref{fig:db-customers}, \ref{fig:db-orders-link}),
one with orders that links to one with customers.  We refer to this as \emph{Extract Entity}
operation, because it introduces a new \emph{entity}, a referenceable holder of mutable attributes.
The operation is needed when we realize that some attributes of one type of entity should belong to
a  distinct type of entity that will be associated with the first one.

The system needs to provide a mechanism for referencing entities, such as by using unique
identifiers (here consecutive numbers). This allows, for example, a spelling error in a customer
address to be fixed in one place. It also allow a customer name or address to change without
breaking the connection from all associated orders. Unique identifiers are the essence of
entities, whether they are uniquely generated primary keys in a database or an abstraction
of a memory address in a programming language.

The \textit{Extract Entity} operation must 1) merge duplicates, 2) assign unique identifiers,
and 3) reference these identifiers from the orders. Our example took the simple approach of
merging entities whose attributes are all equal. But that might not be correct in all cases.
What if there was a typo in one of the addresses that made it look different from other instances
of the customer? To repair that error we need to merge the falsely distinct customers into one
and fix any references from orders. We call this operation \textit{Merge Entities} -- note that
it is not a schema change but a data change, yet nevertheless not commonly supported in data APIs.

More generally, entity evolution can happen through operations such as those listed in
Table~\ref{tbl:dbchanges}. Every schema operation should have an inverse as the
change in requirements that triggered an evolution might change back again. Inverse
is also required in research on \emph{divergence control}~\cite{Foster2007, herrmann17, chillon22}.
The inverse of \textit{Extract Entity} is \textit{Absorb Entity}. It raises the question of how
to handle \textit{one-to-many} relationships. For example should absorbing orders into customers
do a relational join, returning us back to the starting point, or should it nest orders within
customers, as a NoSQL database might do?

The inverse of \textit{Merge Entities} is \textit{Split Entity}, but that can be awkward.
Imagine that we didn't have addresses for customers to disambiguate them with and only knew
their non-unique names. Then \textit{Extract Entity} will conflate them and discard the
information needed to properly invert the operation. Should this information be retained?
One could object that we shouldn't have taken orders for people we can't uniquely identify
yet. And even if that was necessary it means the customers really aren't distinct entities
and shouldn't have been extracted. These are arguments that we shouldn't need \textit{Split
Entity}, contradicting the principle that evolution operations should be invertible.
It is interesting to observe that evolution in nature can also fail to invert, leaving
behind vestigial features. We consider this to be an open issue.

\begin{table}[t]
\begin{tabular}{lll}\toprule
{\firamedium Change} & {\firamedium What} & {\firamedium How}\\\midrule
Extract & new type of entity & Migrate data to a new table and add links \\
Absorb & a linked type of entity & Migrate data and remove unneeded table \\
Merge & multiple data items & Automatic de-duplication with mistake handling \\
Split & multiple data items & Interactively, following user guidance\\
\bottomrule
\end{tabular}
\vspace{0.3em}
\caption{Changes to types and how a programming system should handle them}
\label{tbl:dbchanges}
\vspace{-1em}
\end{table}

\frameworkbox{
In a programming system, the \emph{types} to which entity evolution applies can be data tables,
but also objects or JSON. When the type evolves, the system needs to update corresponding
live \emph{data} and \emph{code}. In a non-local scenario, entity evolution
typically concerns permanent data. As a result, the system needs to synchronize types,
data and code across all replicas. Doing so in a way that allows divergence is discussed
in the next section.
}

\subsection{Remarks}
The central problem is to coordinate the type evolution with code edits to keep the system
executing live and correctly. Solutions may infer the desired type evolution by analysing
source code edits, but would typically offer UI affordances for the commands. They should
try to minimize the number and complexity of commands that must be introduced and also the
interruption to live execution of the code.

The Extract Entity challenge in the non-local case (for local-first software) is similar to the
local live programming challenge except that the context is an application that offers similar
functionality with peer-to-peer collaboration. In this context the schema change does not need
to be performed interactively -- a developer can take some time to build, test, and package a
solution, perhaps using an API or DSL. The hard part comes when the schema change is to
be deployed across all the replicas. This deployment must synchronize code and data upgrades
(or decouple them as in Cambria~\cite{Cambria}). The deployment must also deal with
migrating ``in flight'' operations so that all replicas converge on the same state
without data loss, and ideally without centralized coordination. A radical approach
could try to incorporate schema change operations into the underlying datastore
itself (possibly a CRDT) and integrate code as well, but layered solutions are welcome too.

\challenge{Code Co-evolution for Extract Entity}

In a programming system, there is typically also some code for programmatic manipulation
of entities. The code can, for example, implement a simple CRUD UI that also provides the
Acme shipping department its view of unshipped orders. The holistic perspective of programming
systems lets us see this code not as separate from the system, but as its integral part.
The code thus also needs to co-evolve when the entity evolves. An example is a SQL
query to report all pending orders in the original schema:

\begin{lstlisting}[language=SQL]
SELECT order_id, item, quantity, customer_name, customer_address
FROM Orders WHERE ship_date IS NULL;
\end{lstlisting}

\noindent
After applying the \textit{Extract Entity} operation as described earlier, the code should
be rewritten into the following query that joins the two linked tables:

\begin{lstlisting}[language=SQL]
SELECT order_id, item, quantity, customer_name, customer_address
FROM Orders
JOIN Customers ON Orders.customer_id = Customers.customer_id
WHERE ship_date IS NULL;
\end{lstlisting}

\noindent
The same code transformation would need to be applied to queries written over other
data representations such as JSON. The system can also use mechanism such as a database
view and execute the original query over the view. When applying code migrations,
the system should minimize the need for users to manaually intervene. It should also
minimize the possibilities that developer-written code is subtly incorrect in edge cases,
preferably by reducing the need for developer-written code.


\newpage
~
\newpage

% elm
% database
% divergence
% cambria
% documents
% state machine

% DIVERGENCE IN TRANSITION (because you cannot do big bang update) c.f. Tijs' example

% typically people just blow away the state but Elm actually has demos
% of people doing hot reload without losing state.
% (maybe they never change the type?)

% ELM EXAMPLE
% current example ("completes") is contrived - it is equivalent data
% more interesting is when you lose / add data
% add "completed" with default instead



\section{Divergence Control}

\textcolor{red}{This section was an extension of the Extract Entity challenge but should be lifted up to the top-level discussion of divergence}

\subsection{Context}
Schema evolution becomes more challenging when data, code, and schema cannot all be updated together in an atomic way. In these situations, different versions must coexist and interoperate with one another: for example, edits made in one schema must be applied in another, or code written against one schema must run against data stored in another.

Such situations are common in software engineering. Web backend architectures often put data and code on separate machines, which makes it impossible to update both atomically. To perform upgrades, teams must manually perform multi-step [zero-downtime deployment processes](https://planetscale.com/blog/zero-downtime-rails-migrations-planetscale-rails-gem) to step the database and the code servers forward gradually such that they are compatible with one another at any given step. One example would be deploying a change to add a database column before deploying the code that uses the column.

In the simplest cases, divergence can last only for a brief moments in between zero-downtime deployment steps. But divergence can also occur across longer timescales, especially in situations where the different parts of a system are not all centrally controlled. For example, a public web API may need to support applications using old versions of the API for months or years, since it is not realistic to force all consumers to upgrade immediately. Divergence can even be indefinite -- \citet{Cambria} addresses collaborative applications where different users may want to collaborate on shared data through different tools using different schemas, with no intention of ever converging.

Divergence does not just occur in software engineering; it also appears in end-user workflows. Users frequently make copies of structured documents like spreadsheets and then diverge them by altering both data content and schema. In the case of spreadsheets schema changes include rearrangements to the structure of rows and columns as well as changes to formulas. The problem is that the users want to transfer such data and schema changes between these divergent copies, and they want to pick and choose which of these changes to transfer. In practice such transfer is done manually through copy \& paste. \citet{Basman19} has documented an ecology of emailed spreadsheets. \citet{Burnett14} distilled field observations of these practices in the story of Frieda, which we quote here:

\begin{quotation}

For example, consider ``Frieda'', an office manager in charge of her department's budget tracking. (Frieda was a participant in a set of interviews with spreadsheet users that the first author conducted. Frieda is not her real name.) Every year, the company she works for produces an updated budget tracking spreadsheet with the newest reporting requirements embedded in its structure and formulas. But this spreadsheet is not a perfect fit to the kinds of projects and sub-budgets she manages, so every year Frieda needs to change it. She does this by working with four variants of the spreadsheet at once: the one the company sent out last year (we will call it Official-lastYear), the one she derived from that one to fit her department's needs (Dept-lastYear), the one the company sent out this year (Official-thisYear), and the one she is trying to put together for this year (Dept-thisYear).

Using these four variants, Frieda exploratively mixes reverse engineering, reuse, programming, testing, and debugging, mostly by trial-and-error. She begins this process by reminding herself of ways she changed last year's by reverse engineering a few of the differences between Official-lastYear and Dept-lastYear. She then looks at the same portions of Official-thisYear to see if those same changes can easily be made, given her department's current needs.

She can reuse some of these same changes this year, but copying them into Dept-thisYear is troublesome, with some of the formulas automatically adjusting themselves to refer to Dept-lastYear. She patches these up (if she notices them), then tries out some new columns or sections of Dept-thisYear to reflect her new projects. She mixes in ``testing'' along the way by entering some of the budget values for this year and eyeballing the values that come out, then debugs if she notices something amiss. At some point, she moves on to another set of related columns, repeating the cycle for these. Frieda has learned over the years to save some of her spreadsheet variants along the way (using a different filename for each), because she might decide that the way she did some of her changes was a bad idea, and she wants to revert to try a different way she had started before.
\end{quotation}

\subsection{Example}
The Divergence Control challenge instantiates the problem of divergent state with the example from the Extract Entity challenge. Every month the orders department sends its spreadsheet to the accounting department, which adds the data to a version of the spreadsheet that it has customized for financial tracking. But when the orders department migrates its spreadsheet to extract out customers the accounting department does not want to conform. They could manually convert incoming data in the new schema into their variant of the old schema. But they shouldn't have to. It should be possible to run new data through the schema change in reverse, converting it back into the format the accounting department is used to ingesting. Note that this is not a matter of synchronizing the divergent spreadsheets -- they maintain differently evolved schema.

\subsection{Challenge}
What is needed is a user-friendly mechanism for transferring data changes from one schema to another bidirectionally. In one direction we want to transfer changes made in the new schema into a variant of the old schema. The opposite direction might be needed, for example, if the accounting department makes corrections to customer addresses, which ought to be pushed through into the orders department's new schema.

Bidirectional transfer of changes between divergent copies is similar in some ways to source code version control as in Git~\cite{ProGit}. There is \textit{forking} of long-lived divergent copies. We want to \textit{diff} these forks to see exactly how they have diverged. We want to partially \textit{merge} them by \textit{cherry picking} certain differences. Yet there are also many dissimilarities with Git: our data is more richly structured than lines of text; our schema changes are higher-level transformations on these structures than inserting and deleting characters; and we expect that end-users be able to understand it~\cite{gitless}. The Divergence Control challenge is in a sense to provide ``version control for schema change'' meeting these criteria, with the key technical challenge being the ability to transfer selected differences bidirectionally through schema changes as independently as possible.

\subsection{Goals}
This challenge invites a change of perspective in both live programming and local-first software. Live programming must move from being a solitary activity to a collaborative workflow, and one where the collaboration is not just on editing source code but on live integrated code and data, as in the Smalltalk/Lisp images of old.\footnote{If only we had this in the 90's when Smalltalk had a shot at the mainstream!} For its part, local-first software must move from automatically converging data replicas to also interactively managing long-term divergence. That implies either application state is being directly exposed to the user, or the application code is using an API to present version control affordances. We encourage both communities to think outside the box of Git, which has proven to baffle not only end-users but also a substantial fraction of developers.



\section{diagrams}

\begin{figure}[H]
  \centering
  \begin{tikzcd}[sep = 6em]
    \textsf{query}
    \arrow[d, "\textsf{refactor}_\alpha"{name=L, description}]
    &
    \textsf{schema}
    \arrow[l, "\textsf{constrains}"{above}]
    \arrow[r, "\textsf{constrains}"{above}]
    \arrow[d, red, "\textsf{change}_\alpha"{name=C, description}]
    &
    \textsf{data}
    \arrow[d, "\textsf{migrate}_\alpha"{name=R, description}]
    \\
    \textsf{query}_\alpha
    &
    \textsf{schema}_\alpha
    \arrow[l, "\textsf{constrains}"{above}]
    \arrow[r, "\textsf{constrains}"{above}]
    &
    \textsf{data}_\alpha
    \arrow[Rightarrow, from=C, to=L, "\textsf{propagate}"{above}]
    \arrow[Rightarrow, from=C, to=R, "\textsf{propagate}"{above}]
  \end{tikzcd}
  \caption{Extract Entity}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{tikzcd}[column sep = large, row sep = 5em]
    &
    \textsf{schema}
    \arrow[ld, red, "\textsf{change}_\alpha"{name=L2, near start, description}]
    \arrow[rr, "\textsf{constrains}"]
    \arrow[dd, red, "\textsf{change}_\beta"{name=L, near start, description}]
    &
    &
    \textsf{data}
    \arrow[dd, "\textsf{migrate}_\beta"{name=R, near start, description}]
    \arrow[Rightarrow, from=L, to=R]
    \\
    \textsf{schema}_\alpha
    %\arrow[dd, "\textsf{change}_\beta"{name=L3, near start, description}]
    \arrow[rr, crossing over]
    &
    &
    \textsf{data}_\alpha
    \arrow[from=ur, crossing over, "\textsf{migrate}_\alpha"{name=R2, near start, description}]
    \arrow[Rightarrow, from=L2, to=R2, crossing over, "\textsf{propagate}"{above}]
    &
    \\
    &
    \textsf{schema}_\beta
    \arrow[ld, "\textsf{change}_\alpha \succ\, \textsf{change}_\beta"{name=L4, near start, description}]
    \arrow[rr]
    &
    &
    \textsf{data}_\beta
    \arrow[ld, "\textsf{migrate}_{\alpha\beta}"{name=R4, near start, description}]
    \arrow[Rightarrow, from=L4, to=R4]
    \\
    \textsf{schema}_{\alpha\beta}
    \arrow[rr]
    &
    &
    \textsf{data}_{\alpha\beta}
    %\arrow[from=uu, crossing over, "\textsf{migrate}_\beta"{name=R3, near start, description}]
    %\arrow[Rightarrow, from=L3, to=R3, crossing over]
    &
  \end{tikzcd}
  \caption{Mergeing divergent changes with rebase ($\succ$)}
\end{figure}


\section{Related Work}
\label{sec:related}

Acme Corp has a problem, for spreadsheets don't naturally support relationships between entities.
End-user databases like Airtable~\cite{airtable} and Notion~\cite{notion} support \textit{links}
between tables but do not provide operations that can do an \textit{Extract Entity}. Neither do
the established database products (see Related Work). Acme will need to write code or do a big manual conversion.


\begin{figure}[H]
  \centering
  \begin{tikzcd}[column sep = large, row sep = 5em]
    &
    \textsf{schema}
    \arrow[ld, red, "\textsf{change}_\alpha"{name=L2, near start, description}]
    \arrow[rr, "\textsf{constrains}"]
    \arrow[dd, "\textsf{identity}"{near start, description}]
    &
    &
    \textsf{data}
    \arrow[dd, red, "\textsf{update}_\beta"{near start, description}]
    \\
    \textsf{schema}_\alpha
    \arrow[rr, crossing over]
    &
    &
    \textsf{data}_\alpha
    \arrow[from=ur, crossing over, "\textsf{migrate}_\alpha"{name=R2, near start, description}]
    \arrow[Rightarrow, from=L2, to=R2, crossing over, "\textsf{propagate}"{above}]
    \arrow[rr, red, crossing over, "\textsf{update}_\gamma"{near start, description}]
    &
    &
    \textsf{data}_{\alpha\gamma}
    \arrow[dd, "\textsf{update}_\beta \succ\, \textsf{migrate}_\alpha \succ\, \textsf{update}_\gamma"{near start, description}]
    \\
    &
    \textsf{schema}
    \arrow[ld, "\textsf{change}_\alpha"{name=L4, near start, description}]
    \arrow[rr]
    &
    &
    \textsf{data}_\beta
    \arrow[ld, "\textsf{migrate}_\alpha"{name=R4, near start, description}]
    \arrow[Rightarrow, from=L4, to=R4]
    \\
    \textsf{schema}_\alpha
    \arrow[rr]
    &
    &
    \textsf{data}_{\alpha\beta}
    \arrow[from=uu, crossing over, "\textsf{update}_\beta \succ\, \textsf{migrate}_\alpha"{near start, description}]
    &
    &
    \textsf{data}_{\alpha\beta\gamma}
  \end{tikzcd}
  \caption{Conference Organizer merging schema and divergent data updates}
\end{figure}


\textcolor{red}{[JE] These diagrams are getting hairy. Are they actually helpful?}

\newpage





\newpage

\begin{figure}
\begin{adjustwidth}{-5.5em}{-5.5em}
\centering
\begin{subfigure}[b]{23em}\vspace{0pt}
   \fbox{\parbox{22.6em}{
   \textbf{PROGRAMMING CONFERENCE 2023}\\
   \textbf{Invited speakers}
   \begin{itemize}
    \item Adele Goldberg, adele@xerox.com
    \item Margaret Hamilton, hamilton@mit.com
    \item Betty Jean Jennings, betty@rand.com
   \end{itemize}
   \vspace{1em}
   }}
   \caption{Initial state of the document.}
   \label{fig:conf-orig}
\end{subfigure}
\hfill
\begin{subfigure}[b]{23em}\vspace{0pt}
   \fbox{\parbox{22.6em}{
   \textbf{PROGRAMMING CONFERENCE 2023}\\
   \textbf{Invited speakers}
   \begin{itemize}
    \item Ada Lovelace, ada@rsoc.ac.uk
    \item Adele Goldberg, adele@xerox.com
    \item Betty Jean Jennings, betty@rand.com
    \item Margaret Hamilton, hamilton@mit.com
   \end{itemize}
   }}
   \caption{Added a speaker and sorted the list.}
   \label{fig:conf-add}
\end{subfigure}
\\~\\
\begin{subfigure}[b]{23em}\vspace{0pt}
   \fbox{\parbox{22.6em}{
   \textbf{PROGRAMMING CONFERENCE 2023}\\
   \textbf{Invited speakers}

   \begin{tabular}{| l | l | l |}
     \hline
     \textbf{Name} & \textbf{Email} & \textbf{Who} \\
     \hline \hline
     Adele Goldberg & adele@xerox.com & TP\\
     \hline
     Margaret Hamilton & hamilton@mit.com & JE\\
     \hline
     Betty Jean Jennings & betty@rand.com & JE\\
     \hline
   \end{tabular}
   \\~
   }}
   \caption{Refactored list into a table.}
   \label{fig:conf-tab}
\end{subfigure}
\hfill
\begin{subfigure}[b]{23em}\vspace{0pt}
   \fbox{\parbox{22.6em}{
   \textbf{PROGRAMMING CONFERENCE 2023}\\
   \textbf{Invited speakers}

   \begin{tabular}{| l | l | l |}
     \hline
     \textbf{Name} & \textbf{Email} & \textbf{Who} \\
     \hline \hline
     Ada Lovelace & ada@rsoc.ac.uk & \\
     \hline
     Adele Goldberg & adele@xerox.com & TP\\
     \hline
     Betty Jean Jennings & betty@rand.com & JE\\
     \hline
     Margaret Hamilton & hamilton@mit.com & JE\\
     \hline
   \end{tabular}
   }}
   \caption{Resulting document after change merging.}
   \label{fig:conf-fin}
\end{subfigure}
\end{adjustwidth}
\vspace{0.5em}
\caption{Conference organizer. Initial version of the document with two independent changes
and the final version of the document after the two changes are merged.}
\label{fig:conf}
\vspace{0.5em}
\end{figure}


\section{Conference Organizer: Merging edits in computational documents}
In this section, we consider challenges related to working with a document-based format akin to HTML.
We discuss two variants of the format. In the first format, a document
contains only data. In the second variant, the format is extended with computed values.
A computed value is a node in the document whose value is specified by a formula akin to those
used in spreadsheets. The equations can refer to other nodes in the document, possibly using
an absolute or a relative path and may involve a range of built-in functions, for example to
count the number of elements or to sum a sequence of numerical values.

We assume that the document can be
concurrently edited by multiple users in the same fashion as in local-first software, i.e.,
users may edit a local copy that then needs to be synchronized with changes made by other users.
The edits to the document can be done in a rich-text editor, or by using a more structure-aware
structure editor that can, for example, record changes as a sequence of high-level edit operations.

The concrete challenges are centered around the problem of collaborative conference planning
as illustrated in Figure~\ref{fig:conf}. A number of co-organizers are planning a conference
that will feature talks by several invited speakers. They need to agree on the speaker list,
contact speakers and calculate the conference budget.


\frameworkbox{
In this scenario, the only transient \emph{state} is the result of computed values. The content
of the document corresponds to the \emph{data} pace layer. A change in the data layer may invalidate
the computed value at the state layer. Formulas correspond to the \emph{code} layer and changing them,
again, invalidates the state. The \emph{schema} pace layer is implicit. A change to the document that
modifies content is a mere data change, but restructuring the document is an implicit schema
change. Such schema change that may affect the code pace layer.

The scenario assumes the \emph{decentralized} model. There are multiple program (document)
variants and it should be possible to propagate a change at any pace layer to other program
variants. The challenge may be solved using both the easier \emph{convergence} and the more
challenging \emph{divergence} model.}

\paragraph{Applications.}
One of the earliest systems to combine documents with computational elements
is Boxer \cite{diSessa86}. The challenges in this section apply to a wide range of
systems that can be seen as its successors. The specific scenario discussed here is
inspired by the use of Notion~\cite{notion}, which is a collaborative structured document
editing system. Treating documents as end-user programs has recently
been explored in Potluck~\cite{Litt2023}, which is local-only but includes formulas similar
to those in our challenge.

The challenges in this section are also relevant to work on computational scientific notebooks.
Jupyter \cite{Kluyver2016} uses a simple representation as a sequence of cells and does not have
a mechanism for embedding data in the document and referring to it, but numerous
project such as Nextjournal \cite{Nextjournal21} advance the format. The idea of producing
interactive, computational secientific documents is an active research area. Recent contributions
include Nota~\cite{Crichton2021} and Living Papers \cite{Heer2023}.

\challenge{Merging structured document edits}

First, the organizers need to agree on a list of speakers to invite, coordinate who contacts
whom and track the acceptance of invitations. They start with a document shown in
Figure~\ref{fig:conf-orig}. The challenge is to merge document edits that are done locally
and independently by two co-organizers. Specifically, consider the following two edits:

\begin{enumerate}
\item The first organizer adds an additional speaker to the list and sorts the list of speakers
  alphabetically by their first name. (Figure~\ref{fig:conf-add})
\item The second organizer refactors the list into a table. They split the single textual value
  into a name and an email (using a comma as the separator) and add an additional column for
  tracking which of the organizers should contact the speaker.
\end{enumerate}

\noindent
The system should be able to merge the changes and produce a final table shown in
Figure~\ref{fig:conf-fin}. The resulting table needs to include the additional speaker
created by the first organizer, use the order specified by the first organizer and use
the format defined by the second organizer. The newly added speaker should be reformatted
into the new format. For the newly added ``Organizer'' column, the second user may
specify default value or the newly added row may use an empty value.

\frameworkbox{
In this challenge, the change done by the first organizer is at the \emph{data} layer,
while the change done by the second organizer spans the \emph{schema} and \emph{data} layers.
In implementing the schema change, the second organizer transforms the affected document data.
The challenge is to apply the same transformation to the data independently added by the
second organizer.
}

\paragraph{Requirements.}
The challenge is concerned with merging two sequences edits to a base document.
A solution to the challenge should strive to satisfy the following goals:

\begin{itemize}
\item \emph{Completeness.} It should be possible to semi-automatically merge any two sequences
  of edits. In other words, the merging should always be defined, regardless of what edits the
  users perform, although user input may sometimes be necessary.
\item \emph{Commutativity.} The order of edits should not matter, regardless of their pace layer.
  If users $A$ and $B$ perform edits independently starting from the same initial document, the
  resulting document should be the same if the system treats edits from $A$ as occurring before
  the edits of $B$ and vice versa.
\item \emph{Conflict resolution.} It may not always be possible to merge conflicting edits.
  In this case, the system should interactively ask the user for guidance or, possibly, use
  default behavior specified by the user.
\item \emph{Structure editing.} The system may use a structure editor that provides high-level
  commands for operations such as reordering of elements or refactoring of a list into a table.
  In other words, we recognise that the challenge may not be solvable in a system based on plain
  text editing of document source code.
\end{itemize}

\paragraph{Remarks.}
As discussed earlier, this challenge does not make an explicit distinction between the schema
of the document and data. The implementing system may or may not maintain such distinction.
In our example, adding a new speaker and sorting the list is a mere change of the data, but the
refactoring of a list into a table is a schema change. The system may offer different user interface
for changes at different layers and leverage knowledge about such distinction, for example,
by handling the merging of schema-related and data-related changes differently.


\begin{figure}
\begin{adjustwidth}{-5.5em}{-5.5em}
\centering
\begin{subfigure}[b]{23em}\vspace{0pt}
   \fbox{\parbox{22.6em}{
   \textbf{PROGRAMMING CONFERENCE 2023}\\
   \textbf{Invited speakers}
   \begin{itemize}
    \item Adele Goldberg, adele@xerox.com
    \item Margaret Hamilton, hamilton@mit.com
    \item Betty Jean Jennings, betty@rand.com
   \end{itemize}
   \vspace{1.5em}
   \textbf{Conference budget}\\
   Travel cost per speaker:\\
   \hspace*{2em} \$1200 \\
   Number of speakers: \\
   \hspace*{2em} \texttt{=COUNT(/ul[id='speakers']/li)} \\
   Travel expenses:\\
   \hspace*{2em} \texttt{=/dl/dd[0] * /dl/dd[1]}
   }}
   \caption{Initial state of the document with formulas.}
   \label{fig:conf-budget}
\end{subfigure}
\hfill
\begin{subfigure}[b]{23em}\vspace{0pt}
   \fbox{\parbox{22.6em}{
   \textbf{PROGRAMMING CONFERENCE 2023}\\
   \textbf{Invited speakers}

   \begin{tabular}{| l | l | l |}
     \hline
     \textbf{Name} & \textbf{Email} & \textbf{Who} \\
     \hline \hline
     Ada Lovelace & ada@rsoc.ac.uk & \\
     \hline
     Adele Goldberg & adele@xerox.com & TP\\
     \hline
     Betty Jean Jennings & betty@rand.com & JE\\
     \hline
     Margaret Hamilton & hamilton@mit.com & JE\\
     \hline
   \end{tabular}

   \vspace{0.5em}
   \textbf{Conference budget}\\
   Travel cost per speaker:\\
   \hspace*{2em} \$1200 \\
   Number of speakers: \\
   \hspace*{2em} \texttt{=COUNT(/table[id='speakers']/tbody/tr)} \\
   Travel expenses:\\
   \hspace*{2em} \texttt{=/dl/dd[0] * /dl/dd[1]}
   }}\caption{Resulting document after schema change merging.}
   \label{fig:conf-finfin}
\end{subfigure}
\caption{Conference organizer. Formulas to calculate the budget have been added to the
original document (Figure~\ref{fig:conf-orig}) and those have to be merged with the
refactoring of a list into a table (Fig~\ref{fig:conf-tab}). Formulas in the final version
are updated to match with the new document structure.}
\label{fig:conf-calc}
\end{adjustwidth}
\end{figure}


\challenge{Applying schema edits in presence of code}

This challenge extends the previous problem. The conference organizers now also want to use
the document to manage the conference budget. In a very simplified form, one aspect
of this is calculating the estimated travel expenses for the speakers. For this,
they add a computed value to the document that counts the number of speakers
and multiplies the result by a fixed cost per speaker elsewhere in the document.

The scenario is shown in Figure~\ref{fig:conf-budget}. Assume two co-organizers start with
the same original document shown in Figure~\ref{fig:conf-orig}. One adds an additional
section with computed values, while the other performs the edits discussed earlier.
The challenge is, again, to merge the edits done independently by the co-organizers.

\begin{enumerate}
\item The first co-organizer adds the budget calculation as shown in Figure~\ref{fig:conf-budget}.
  This includes two formulas. The first selects all \texttt{li} elements of a \texttt{ul} element
  with ID \texttt{speakers} (not visible in the UI) and counts their number. The second formula
  multiplies the constant travel cost per speaker by the number of speakers. Here, we assume the
  new section uses the HTML definition list \texttt{dl} and the formula selects its first and
  second \texttt{dd} item, respectively.

\item The second co-organizer performs the refactoring and data edits discussed previously.
  They edit the speakers and refactor the structure to use a table, resulting in the document
  shown in Figure~\ref{fig:conf-fin}.
\end{enumerate}

\noindent
As before, the system should be able to merge the edits made to the document.

\frameworkbox{
In this challenge, the change done by the first organizer is at the \emph{code} layer, while
the change done by the second organizer is both at the \emph{data} layer and, implicitly,
at the \emph{schema} layer. A change solely at the \emph{data} layer would not affect code,
but a change at the \emph{schema} layer requires adapting some aspects of the code.
}

\paragraph{Remarks.}
Addressing the challenge requires that the refactoring of the document structure done in (1),
is also reflected in the code of the equations added in (2). As shown in
Figure~\ref{fig:conf-finfin}, in this specific case, the system needs to change the equation
\texttt{COUNT(/ul[id='speakers']/li)} to \texttt{COUNT(/table[id='speakers']/tbody/tr)}.
This is the case because the refactoring consists of four steps that each transform the
document and also need to update the code of the equation accordingly:

\begin{itemize}
    \item[--] Change the type of the \texttt{ul} element to \texttt{table}
    \item[--] Wrap the body of the \texttt{table} element with \texttt{tbody}
    \item[--] Change the type of all children of \texttt{tbody} from \texttt{li} to \texttt{tr}
    \item[--] Further split the body into two \texttt{td} elements
\end{itemize}

\noindent
It is expected that the system solving the challenge will need to be aware of those high-level
edits. In particular, they may be explicitly specified by the user (through an interactive
user interface for editing the document) or semi-automatically inferred.

\paragraph{Requirements.}
A solution to the challenge should meet the goals of the preceding challenge, i.e., completeness
(merging should be always defined), commutativity (order does not matter), conflict resolution
(ask for guidance if needed), convergence (everyone eventually wants the same document) and
structure editing (the system may capture high-level edits).

\subsection*{Challenge Extensions: Liveness and divergence}
So far, we assumed that the system follows the \emph{convergence} model, i.e., all variants
of the program eventually adopt all changes. A more difficult variant on the challenge is to also
support the \emph{divergence} model. In this case, some of the users continue using their variant
of the program without adopting all of the changes done by other users.

\frameworkbox{
In the \emph{divergence} model, it may be expected that users will want to adopt all changes
done at the \emph{data} pace layer. They may selectively choose some of the changes at the
\emph{code} and \emph{schema} layers. The challenge is maintaining the same data across multiple
program variants and merging changes done to the data based on different document schema.
}

The basic challenge outlined above focuses primarily on the local-first software scenario, but
it can be extended to also apply to the live programming scenario. In particular, when the
document contains computed values, those may be evaluated either explicitly by the user or
automatically on-the-fly. An edit can then invalidate some of the values (either by changing
the data that a computed value depends on, or by changing the equation used to compute the value).
A live programming system should be able to detect which computed values are affected by an edit
and, either invalidate those (requiring an explicit re-evaluation by the user) or automatically
re-evaluate them (and possibly highlight the affected values to inform the user).

\frameworkbox{
In the live extension, we also consider the transient \emph{state} pace layer. A system should
automatically detect when a change at the \emph{data} or \emph{code} layers affects the computed
value and forces it to be recomputed. Interestingly, a change at the \emph{schema} layer may not
always affect the \emph{state} layer, even if it schema change requires modifying formulas that
compute the state.
}


\newpage

\section{Live Modeling Languages Require Run-Time State Migration}
\label{livemodeling}
%Related work
%``Slogan'': editing a program is diverging from run-time schema.

\subsection{Context}
In stateful live programming, a change in the program can make the run-time state of the execution out of date, hence we want to migrate the run-time state to keep on running.

One can see a program itself as an instance of a static schema (its AST type), that in turn determines (defines/implies/induces) a run-time schema: the run-time structures of code (e.g., classes, inheritance links, declarations, method definitions etc.), as well as the structure of the run-time state. Whereas the run-time \textit{code} structures do not typically change while using an application, the run-time \textit{state} constantly changes.
Live programming, however, requires reconciling changes to the program with the run-time structures of both the code and the state. This typically means that at a certain point (a quiescent point) during execution, the code structures need to be updated (hot swapped), and the state needs to be \textit{migrated}.

\subsection{Example}
Consider the example of a simple state machine language with on-entry actions, and typed global variables (loosely inspired by the SML language of \citet{vanRozen19}). An example state machine and an excerpt of its abstract syntax schema is shown in Figure~\ref{LST:statemachines}. Depicted on the left, an actual statemachine modeling the opening and closing of a door, with one global boolean variable, \lstinline{isClosed}, which is flipped according to transitioning between the \lstinline{closed} and \lstinline{opened} states. For the sake of brevity, the abstract syntax schema (similar, to e.g., and Ecore metamodel~\cite{EMF}) shown on the right of the figure omits classes for the on-entry actions (\lstinline{Stmt}) and variable types (\lstinline{Type}).

\begin{figure}[t]
\centering
\begin{minipage}[t]{0.4\textwidth}
\begin{lstlisting}[language=java,morekeywords={machine,on,state,var}]
machine Doors

var isClosed: bool = true

state closed
  isClosed := true
  on open => opened

state opened
  isClosed := false
  on close => closed
\end{lstlisting}
\end{minipage}
\hspace*{5pt}\vline
\begin{minipage}[t]{0.4\textwidth}
\begin{lstlisting}[language=java,morekeywords={on}]
  class Machine
    name: string
    states: State*
    vars: Decl*

  class State
    name: string
    onEntry: Stmt*
    transitions: Trans*

  class Trans
    event: string
    target: State

  class Decl
    name: string
    type: Type
  \end{lstlisting}
\end{minipage}
\caption{A statemachine and its abstract syntax schema (omitting \lstinline{Stmt} and \lstinline{Type})}
\label{LST:statemachines}
\end{figure}

In the following we assume that the state machine is executed by an interpreter that process an incoming stream of events, and traversing run-time pointers representing transitions accordingly, executing on-entry actions, where the notion of the current state and the values of the global variables are the collective run-time state of the program. The structure that the interpreter operates on can be described as derived  schema from the abstract syntax scheme shown on the right of Figure~\ref{LST:statemachines}, but specialized for a particular machine, e.g., \lstinline{Doors}.
This schema will have additional fields and associations modeling the run-time data required for execution. For the \lstinline{Doors} statemachine, this means adding the current state to the \lstinline{Machine} type (required for any machine) and the machine specific data fields, namely \lstinline{isClosed}. The run-time schema for \lstinline{Doors} is then the following\footnote{It is possible to represent the global variables at run-time using an untyped table or hashmap, but for the sake of schema migration complexity, the encoding as fields in classes is more instructive.}:

\begin{lstlisting}[language=java,morekeywords={on},mathescape=true]
class Machine$^{++}$
  states: State*
  current: State
  isClosed: bool
\end{lstlisting}

This \textit{extended} class $\text{\lstinline{Machine}}^{++}$ also represents state machines, but this time at run time.



\begin{figure}[t]
  \centering
\includegraphics[width=0.5\textwidth]{figures/doorsmachine.pdf}
\caption{UML object diagram of the run-time structures of the Doors statemachine (excluding code objects)}
\label{FIG:doorsRuntime}
\end{figure}


An instance of a running state machine consists of an object graph conforming to the run-time schema of state machines, updating the \lstinline{current} state pointer in response to events. This object graph is graphically depicted in Figure~\ref{FIG:doorsRuntime} as a UML object diagram, excluding the statement objects representing on-entry actions. As the diagram shows, the running machine has a current state (\lstinline{closed}) and the \lstinline{isClosed} field has an actual value (\lstinline{true}). It is instructive to note that the diagram encode both static and dynamic aspects of the state machine language. Live editing the state machine then requires to \textit{patch}~\cite{SemanticDeltas} this run-time structure without shutting it down, possibly requiring migration of run-time state.

\subsection{Challenge}

\begin{table}[t]
  \centering
\begin{tabular}{lll}\toprule
\textsc{Change} & \textsc{What} & \textsc{How}\\\midrule
add & state & simple  \\
remove & state & structurally simple, but heuristic if state is current \\
rename & state & simple\\
add & variable & migrate class and initialize field \\
remove & variable & migrate class (NB: assumes variable is unused)\\
rename & variable & rename in class, preserving value\\
type change & variable & migrate class and convert or reinitialize value \\
add & transition & simple \\
remove & transition & structurally simple, but heuristic for pending events\\
event change & transition & structurally simple, but heuristic for pending events\\
add/remove & statement & hotswap code at quiescent point of interpreter\\
\bottomrule
\end{tabular}
\caption{Possible program changes and how to deal with them at run time}
\label{tbl:smchanges}
\end{table}

The space of possible (well-formed\footnote{The abstract syntax schema is not expressive enough to define all static invariants of the language, such as: states must by unique by name, references variables in actions must be declared, etc.}) changes to a state machine are summarized in Table~\ref{tbl:smchanges}.
The first column indicates the change category, the second the affected kind of object, and third a short description of how to deal with the respective change category. Some notes about this table:
\begin{itemize}
\item all rows with ``simple'' in the third column only require a quiescent~\cite{Tranquility} point in the interpreter loop to update the run-time structure (e.g., as shown in Figure~\ref{FIG:doorsRuntime}).
\item removing a state, however, is only \textit{structurally} simple, since removal is easy, but special care is required if the subject of removal is the current state. In this case, some heuristic is needed, such as: reject the edit, point the current state to the initial state, or some other strategy (e.g., the nearest state, previous state etc.)
\item everywhere a row mentions ``migrate class'' in the third column, data migration is needed: the run-time objects conforming to the old class must be transformed to instances of the updated class, similar to how Smalltalk migrates objects using \lstinline{become:}.
\item removing a transition or changing the event of a transition potentially has to deal with pending events (e.g., in an event queue) expecting such transitions, since such events are now potentially stale. Strategies to deal with this situation include: simply dropping the events, or requiring that such edits cannot be patched at run time when there are pending events.
\item the type change edit of variable requires a strategy for the current value: either discard and reinitialize, or perform value conversion. For instance, if the type change is from boolean to integer, then true could become 1, and false could become 0.
\item note finally that rename variable is mentioned explicitly as a change: this makes it possible to preserve the run-time value of the variable.
\end{itemize}

\subsection{Goals}

The goals for this challenge are twofold: from the end-user perspective and from the language engineering perspective. A particular challenge from the end-user perspective is to ``do minimal harm'': it is essential for fluid programming experience that the automatically triggered migrations are in a sense as near as possible to the previous application state, to not surprise or confuse the user/developer.
One approach  considered in earlier work~\cite{RuntimeConstraint} employs the constraint solver Z3 to find a ``nearest'' run-time instance compatible with a source change. Nevertheless, the patching of the run-time state should be quick enough so as to not disrupt the programmer experience.

From the language engineering perspective the goal is to employ techniques, formalisms, and tools, to make the construction of such languages easier. The above example state machine DSL is derived from earlier work~\cite{vanRozen19}, where the authors manually implemented the run-time patch operation and concluded that even for such a simple language (simpler even than the example above) it is a complex and error-pone endeavor. Furthermore, the field of software language engineering studies and develops generic and reusable techniques to improve the development of DSLs and programming languages, for instance, in the context of language workbenches~\cite{ERDWEG201524}. The development of live programming languages, however, is currently still out of reach for all existing language workbenches. The aforementioned approach using a constraint solver is an example of such a \textit{language parametric} technique, in that it operates on the (extended) abstract syntax metamodel of a language, and does not assume anything further about the language itself. Another approach is the \textsc{Cascade} metamodeling formalism which has builtin support for run-time patching~\cite{Cascade}. However, further research is needed to design better principled language engineering approaches that solve the problem in a way that is both declarative and fast.

\subsection{Extensions}

While the above example is arguably simple, the problem becomes much more challenging when the programs themselves define data types, classes, records, structures etc. Since possibly many instances (values, objects) of such data types may exist at run-time, these all have to be migrated in such a way that programmer experience is minimally disrupted, and that the invariants of said data types is maintained.
Another extension, tying in with the data-oriented examples above, involves refactorings of data types in a program. Typically a refactoring should be behavior preserving, but can it also preserve run-time data? The minimal example is the consistent variable rename rename in Table~\ref{tbl:smchanges}, which should not have any effect on run-time state.  A more complex refactoring is described in Section~\ref{sec:elm}.


\section{Multiplicity change}

\subsection{Context}

A common kind of schema change is \textit{multiplicity change}: when a field changes from storing a single value to storing multiple values. For example, we might start by storing a single address for each contact in an address book, before realizing that we need to store multiple addresses for a single contact; or we might assign each todo in a list to one person before realizing we want the ability to assign a todo to multiple people.

In a relational schema, solving this problem might require extracting a normalized table for the linked entity; some of the challenges of this approach are covered in the Extract Entity challenge. In this section we will instead consider a document schema with support for arrays; in this context, we can have a multiplicity change by turning a scalar value into an array value.

Multiplicity change clearly requires changing both the data and the code. It becomes particularly challenging to handle when there is ongoing divergence between multiple versions of the data.

\subsection{Example}

Consider the following schema for a todo list item, in which each item has a single assignee, represented by a user ID:

\begin{lstlisting}[language=ml]
type Item = { id : id; title : string; assignee : string }
\end{lstlisting}

Now, we change the schema so that each item has a list of assignees:

\begin{lstlisting}[language=ml]
type Item = { id : id; title : string; assignees : Array<string> }
\end{lstlisting}

Our goal is to preserve the ability for actors in the system to read and write to a shared todo list in either the old or new schema -- for example, they should be able to write to either the scalar assignee field or to the list of assignees.

A natural invariant to preserve across these schemas is that the value of the scalar field should equal the first element of the list field (and if the list is empty, the scalar field should be null). If we were to write code for a one-time data migration from the scalar to list schema, we could easily satisfy this invariant:

\begin{lstlisting}[language=ml]
item.assignees = if (item.assignee == null) then [] else [item.assignee]
\end{lstlisting}

Ongoing edits to the array schema can also be handled in a straightforward way. After edits are made, the value of the scalar field should be set to the first element of the new array (or null if the new array is empty.)

However, handling edits to the scalar schema presents more of a challenge. Consider the following todo with two assignees, presented in terms of the scalar and array schemas. A write is made in the scalar schema to set the new assignee to C.

\begin{lstlisting}[language=ml]
todoScalar = { id: 1, title: "Foo", assignee: A }
todoArray = { id: 1, title: "Foo", assignees: [A, B] }

todoScalar.assignee = C
\end{lstlisting}

What should the array value become after this edit to the scalar schema? To satisfy our invariant, we know that C must become the first element of the array, but this leaves open several options with different tradeoffs:

\begin{enumerate}
  \item \texttt{[C]}: "Only C should be assigned." This option produces an array that corresponds directly to the resulting scalar. But it has the downside of deleting data that wasn't even visible in the scalar schema.
  \item \texttt{[C, B]}: "Replace A with C." If the writer wanted to remove A from the assignment and add C, this option performs that intent. However, there is data remaining in the list which was not visible to the scalar writer.
  \item \texttt{[C, A, B]}: "Add C to the list." Perhaps the scalar writer wanted to add C to the assignment without removing anyone; this option satisfies that intent. But it preserves data not visible to the scalar schema.
\end{enumerate}

Because the intent of the writer to the scalar schema cannot be unambiguously interpreted from the write alone, it is impossible to make a perfect choice among these options.

\subsection{Remarks}

If schemas, code and data can all be updated atomically together, then multiplicity change is relatively straightforward  to handle. Existing scalar data can be trivially migrated to a list, and the code using the data will need to change to accommodate the list data.

However, ongoing divergence makes multiplicity change much more difficult, and exposes some general challenges. Different schemas may expose partial information, and writers in those schemas have to operate without total knowledge of the information available in other schemas. As a result, synchronizing writes across the schemas requires making difficult tradeoffs, such as choosing to either preserve or destroy hidden data not visible to the writer.

Although there is likely no silver bullet to navigate these tradeoffs, a good solution to this problem would give developers or users tools to manage these tradeoffs. For example, a system might allow developers to specify the desired behavior for a particular pair of schemas based on the requirements of the domain. Or a system might even allow users to manually disambiguate their intent for a given write.



\section{Related Work}

TODO: Maybe insert something about methodology in general?
(ref the table challenges paper? although this paper is no longer so much about challenges)

\paragraph{Schema evolution.}
Schema evolution has long been a major problem for SQL databases, which provide surprisingly little help.
SQLite \cite{sqliteDatatypes} uses dynamically typed values allowing them to be implicitly converted if the datatype of the column changes.
MYSql \cite{mysqlAlterTable} can reorder columns without destroying their data. But apart from such special cases, SQL databases have no general purpose support for data migration. As a result in practice schema evolution is still mostly done by writing custom SQL code to alter the schema and migrate the data. Such custom code is greatly complicated if it needs to be done without taking the database offline or must be coordinated across multiple shards.
In \emph{Refactoring Databases} \citet{ambler06} offer a comprehensive taxonomy of schema evolution patterns, including typical strategies for online migration and sample SQL implementations.

\citet{bernstein07} observed in 2007 ``There are hardly any schema evolution tools today. This is rather surprising since there is a huge literature on schema evolution spanning more than two decades.'' There are now more tools available. Some tools such as Liquibase~\cite{liquibase} and PlanetScale\cite{planetscale} could be characterized as version control and continuous integration/deployment for schema. They track schema changes and can calculate diffs in the form of SQL DDL statements to convert one schema version to another, but do not help migrate data. Unfortunately comparing schemas can be ambiguous about the intention of changes. For example has a column been renamed or has it been deleted and a new one created? That distinction makes a big difference to the data in that column. EvolveDB\cite{evolvedb} addresses this ambiguity by reverse-engineering the schema into a richer data model and tracking the edits to that model within an IDE. This more precise edit history can be used to infer higher level intentions of a schema change, which then generate SQL scripts to evolve the database. EdgeDB~\cite{edgedb} resolves ambiguities by asking questions of the developer, with some answers supplying custom migration code in a proprietary query language.

Some tools provide a Domain Specific Language (DSL) to describe schema evolution. Rails Migrations~\cite{RailsMigrations} embeds a DSL in Ruby to manage schema migration but is comparable to the capabilities of SQL DDL, often requiring the addition of custom Ruby or SQL code. \citet{curino08} spawned a stream of research on Database Evolution Languages (DEL) by defining a Schema Modification Operator (SMO) as ``a function that receives as input a relational schema and the underlying database, and produces as output a (modified) version of the input schema and a migrated version of the database''. SMOs can also rewrite queries to accomodate schema changes. \citet{herrmann15} defined a relationally complete DEL and then extended it into a Bidirectional Database Evolution Language (BiDEL)~\cite{herrmann17}. BiDEL appears capable of handling the basic requirements of our \textit{Extract/Absorb Entity} and \textit{Multiplicity Change} challenges, though \textit{Split/Merge Entity} is less clear. It provides schema divergence by supporting multiple schema within one database, but would need some extension to handle data divergence.

\citet{wang19} use program synthesis to rewrite SQL programs to adapt to a schema change. This process is driven by finding possible correspondences between columns in the old and new schema guided by heuristics on textual similarity and confirmed by the existence of a synthesized equivalent program.
\begin{comment}
  I hate this paper - incredibly sophisticated techniques applied to an over-idealized problem. It is assumed the migration does not change datatypes or lose any information visible to the programs. It assumes the database has full code coverage. It ignores without explanation ambiguous schema changes (for example renaming equityped columns).
\end{comment}

Schema evolution is also a problem for NoSQL~\cite{sadalage12} databases. While such databases are sometimes called ``schemaless'' in effect that means the schema is left implicit and tools must try to infer it~\cite{storl20, storl22}. \citet{Cambria} uses lenses~\cite{Foster2007} for bidirectional transformation of JSON. \citet{scherzinger13} define a set of operators like the relational SMOs discussed above. \citeauthor*{chillon21}~\cite{chillon21, chillon22} offer a more comprehensive set of SMOs that may be capable of handling \textit{Extract/Absorb Entity} and \textit{Multiplicity Change} but not \textit{Split/Merge Entity} nor divergence. None of the above approaches to NoSQL evolution have yet extended into updating code or rewriting queries like their SQL cousins.

\emph{Local-first software}~\cite{localfirst} adds the divergence dimension to NoSQL evolution, because the code running on different replicas can evolve at different paces. \citet{Cambria} applies lenses~\cite{Foster2007} to this problem, specifically discussing the \emph{Multiplicity Change} problem.

Schema evolution has also been studied for Object Oriented Databases(OODB)~\cite{li99,banerjee87}. Smalltalk~\cite{Goldberg80} is itself an OODB, persisting all object instances in an ``image file'' with some evolution capabilities incorporated in the programming environment~\cite[pp.252-272]{Goldberg80}. Gemstone turns the Smalltalk image into a production-quality database and accordingly provides a complex schema evolution API~\cite{Gemstone}.

\paragraph{Type-driven transformations.}
Programming language theory undergirds research on type-driven transformations. Bidirectional Transformations~\cite{czarnecki2009bidirectional} have been used for data format conversion and view update.
Coupled Transformations~\cite{Berdaguer07, alcino06, Cleve2006} express transformations coordinated between types and their instances by encoding them into functions on Haskell GADTs built with strategy combinators. \citet{JVisser08} extends the encoding to transform queries written in point-free style. \citet{lammel16} recapitulates coupled transformations within logic programming which extends it to transform logic programs. It is not clear how bidirectional transformations can handle generating the unique IDs required in some evolutions, nor the information loss and duplication which can arise in some divergence scenarios.

\paragraph{Model Driven Engineering.}
Model Driven Engineering must also handle evolution. Unlike the textual artifacts involved in other domains, models typically assign unique indentifiers enabling more precise differencing and mergeing~\cite{alanen2003}.
Models are often themselves modeled by a metamodel, which lifts the evolution problem to the metalevel: models must be migrated through the evolution of their metamodel, analogously to changing the grammar of a programming language.~\cite{Herrmannsdoerfer11}. \citet{Cicchetti11} study metamodel evolution in the context of divergent changes.
\citet{vermolen11} define a DSL for evolving a data model by generating SQL migrations on the backing database. They consider the evolution of OO-style subclass hierarchies which goes beyond most other work. Their running example includes both our Extract Entity and Multiplicty Change examples, and would make for a good follow-on problem. However they do not address code/query rewriting.

\paragraph{Migration by example.}
Programming by example has been used to infer schema migration programs from examples specified by the developer~\cite{wang20, Alexe11}.
This work deals with the reality that schema live in the database and so examples must be supplied separately, but we wonder why not incorporate examples into a schema definition language? Textual edits to schema in this language would also adapt the examples, which then could be used to infer an abstract migration, thus avoiding the ambiguities of textually comparing pure schema.

\paragraph{Live programming.}
Live programming~\cite{tanimoto90, rein2018exploratory} seeks to speed up feedback in (mostly solo) programming.
% \cite{mcdirmid13, Rauch_2019, beckmann2021shortening}
% A special case of live programming is \emph{hot reloading} (also called \emph{hot swapping})~\cite{barenz2020essence, } that installs code changes into a running system preserving state but not attempting to migrate that state through type changes.
\citet{SemanticDeltas} proposes a research agenda for thoroughly live DSL programming focusing on \emph{Semantic Deltas} that unify edit-time and run-time change.
\citet{RuntimeConstraint} study the live modeling problem in Section \ref{livemodeling}. They use constraints supplied by the developer to solve for the best new runtime state following a DSL language evolution. \citet{vanRozen19} take an alternative approach to the same problem using \emph{origin tracking} in textual editing.
%\citet{vanRozen23} subsequently proposed a metaprogramming language for live DSL programming.


%\paragraph{collaborative programming tools}\cite{goldman2011real,kurniawan2015coder,replit}


%\bibliographystyle{abbrv}
%\bibliography{paper}
\printbibliography


\appendix

\section{\textcolor{red}{[TVDS] Super rough attempt at the paragraph}}

Modern day software development is collaborative. Single user programming does not exist anymore.
Just as document editing is becoming more collaborative through online systems like pioneered by Google Docs,
programming would benefit form online collaboration. In a sense this is live programming++: whereas traditional
live programming systems assume a single developer editing code and see their changes immediately reflected
in the running system, online collaborative programming is the same but for multiple programmers at the same time.
The key challenge here, we posit, the *schema change*. We take inspiration from database migration,
model evolution, and version control, and identify benchmark problems. This is already evident in live programming:
editing a program, consisting of type definitions defining the structure run-time state and (transient) procedures, functions
etc. operating on that state, requires run-time state to be migrated, after the programmer edits their program.
Adding the dimension of multiple programs brings us into the domain local-first, and decentralized version control, where
copies are created and edited, which eventually have to be reconciled again (e.g., through merging).
Whereas schema evolution and its associated migration problems has seen a lot of attention in the database world,
the model-driven world, and the bidirectional transformation (BX) communities, the techniques are still too low-level,
too manual, and too much focused on a single user perspectives. By identifying the similarities between analogous
problems in live, multi-user programming systems, we hope to stimulate further research on more comprehensive, automatic,
high-level, and online solutions. So that the benefits of live programming can be enjoyed in a global and distributed setting
involving multiple programmers at the same time.

\section{\textcolor{red}{[JE] Type evolution for live programming}}

Many past improvements in the practice of programming worked by speeding up feedback loops. Live Programming aspires to the optimum of immediate feedback. The problem is that modern software is built out of many layers of different technologies -- thoroughly live programming would propagate changes immediately up and down the entire stack of a running system, from the database to the code, and from the client to the server. We call this \textit{full-stack} live programming. Another problem is that modern software is built with complex collaborations involving development, testing, deployment, and suppport. Such workflows are themselves feedback loops -- thoroughly live programming would propagate changes between participants frictionlessly. We call this \textit{end-to-end} live programming.

We propose a research agenda towards this greater vision of live programming. The starting point is database \textit{schema evolution}, which propagates database schema changes into data migrations and query rewriting. We call for generalized \textit{type evolution} that works immediately across the entire software stack, and automatically across the entire software workflow. This paper contributes a set of challenge problems to serve as targets for research on type evolution and as a basis of comparison for the results.

\section{\textcolor{red}{[GL] Rough paragraph}}

Most programming languages analysis treats state as ephemeral: tied to the lifetime of a running program, and reset when the program is edited. But when we take a broader view, most \textit{programming systems} (cite) have state that lasts beyond the lifetime of a single execution. This includes collaboration applications with state persisted in a database, image-based systems like Smalltalk, and live programming environments where the program can be edited without resetting all state.

Although these programming systems exist in very different contexts, they all encounter similar problems related to the essential difficulty of preserving state across program edits. New versions of the program must correctly handle pre-existing data. When the data schema or type definitions change, existing data must be evolved to conform to the new schema. Changing data representations or types can also require changing corresponding parts of the program. These problems have been noticed, named, and studied in these different programming systems: ``schema evolution'' in the databases literature, ``hot reloading'' in live programming, etc.

In this paper, we draw connections between these different contexts, pointing out essential shared structure in the hopes of motivating further research on this important problem and encouraging knowledge sharing across subfields. Our contributions are: 1) a shared framework for thinking about the relationship between schema, code, and data in a programming system, 2) Case studies of the framework in a highly diverse set of programming systems, motivated by real-world examples.


\section{\textcolor{red}{[TP] Rough paragraph}}

Programming is increasingly collaborative and interactive. We welcome this trend, because
greater collaboration and interactivity make programming more effective. Future programming
environments enable programmers to collaborate at a more fine-grained level, adopting individual
code changes created by their collaborators as needed. They also let programmers interactively
edit code, often during program execution.

In order to support this new way of working, programming systems increasingly have to tackle the
problem of schema evolution. If a programmer imports a change that edits a schema, code and data
that relies on such schema needs to be adapted to match the new schema. Similarly, if a programmer
in a live programming environment modifies their type definition, current program state needs to
be updated to match the new structure of types. This is a well-understood and well-studied problem
in database community, but it has received little attention in work on programming systems.

The aim of this paper is to provide a conceptual framework for talking about schema evolution in
collaborative and live programming systems. Taking inspiration from Stewart Brand's analysis of
pace layers, we propose to think of the problem as the problem of evolution of occuring at three
layers, the most permanent schema layer, less permanent code layer and least permanent data layer.
A change at the most permanent layer affects the less permanent layer, which then has to be
updated to keep correspondence between the layers.

We consider four case studies of such schema evolution in collaborative and live programming
systems. We present those throught the perspective of our unified conceptual framework. The work
both sheds a new light at how schema change needs to be handled in programming systems and also
provides a range of challenges that show interesting problems that future programming systems
will need to tackle.


\section{Introduction}

\textcolor{red}{[JE] We should adopt the term Schema Evolution, which is favored in the research literature, defined to include schema change, data migration, and query rewriting. }

Schema change won't go away. Changing requirements and changing code lead to changing the schema of a database. Schema change, also called schema migration, is the problem of migrating existing data from the old schema to the new. This often involves custom migration programs or specialized Domain Specific Langiuages. The migration must be carefully coordinated with upgrading the application code and associated artifacts that assume the new schema. Schema change on database servers is often delegated to Database Administrators and DevOps. Live programming~\cite{tanimoto90,Hancock03} and local-first software~\cite{localfirst} both move data away from the server, eliminating those jobs, but not eliminating the need to do schema change, and indeed increasing the need to do it automatically. Recently \citet{Cambria} spotlighted these problems and proposed an approach using lenses~\cite{Foster2007}, but otherwise there has been surprisingly little research. To promote further progress we offer a set of challenge problems to the live programming and local-first communities, inviting them to propose and compare solutions.

Live programming seeks to erase the boundary between editing and running programs. In order to do so program data must be kept around while the program is being edited. Classic Lisp and Smalltalk systems integrated code and data into a persistent \textit{image}~\cite{Sandewall78, Goldberg80}. An interactive shell or \textit{Read Eval Print Loop (REPL)}~\cite{Deutsch64} is more transient than an image, but still builds up a context of data over long-lived programming sessions the loss of which disrupts the programmer's workflow. In all of these environments programs eventually get changed to create and expect data in a form incompatible with extant data. This happens whether or not the form of the data is specified explicitly in a type system or database schema, or whether it is left implicit in the behavior of the code. Some languages can tolerate a larger set of such changes, most notably Smalltalk which will automatically insert and delete members in existing instances when a class definition is changed~\cite[pp.252-272]{Goldberg80}.\footnote{Gemstone turns the Smalltalk image into a production-quality database and accordingly provides a sophisticated schema change API~\cite{Gemstone}. Schema change won't go away.} But there is still a large class of changes that create incompatibilities with existing data, whether it is inside an image, programming environment, REPL, or an external database. Some live programming environments generate data with unit tests, but that only shifts the problem of schema change to adapting those tests. One way or another, stopping everything to manually deal with schema change contradicts the goals of live programming. Schema change won't go away.

Local-first software faces related problems. Its goal is to empower users by moving code and data from the cloud to the user's own devices. Distributed programming techniques like Convergent Replicated Data Types (CRDTs)~\cite{Shapiro11} are used to coordinate data changes peer-to-peer. Unfortunately these techniques so far have not addressed schema change nor code deployment The traditional techniques of schema change used in centrally managed databases are complicated by the distributed and intermittently connected nature of local-first data. Schema change won't go away.

In the following sections we present a series of challenge problems dealing with schema change in the context of live programming and local-first software. These problems are necessarily expressed using established idioms or conventions, but nevertheless we welcome solutions that translate the spirit of the problem into other contexts.

~

~

\textcolor{red}{TODO: Need to explain that we take the programming system view - program is not something
built centrally, but something that exists and can be modified}

~

\textcolor{red}{TODO: Maybe the right way to frame this is to say that the challenges are intended
less for evaluating particular solutions but more for framing and making sense of different
kinds of schema-change-related problems.  You can read this paper and when you face a schema-change-related
problem, think about it in terms of our dimensions. Maybe compare it to our challenges.
We do not necessarilly expect that people will directly solve our challenges though. (Although
some might and that would be nice..)}

~

\textcolor{red}{TODO: I also guess this is more about just ``change'' than ``schema change''
which is one particular (most challenging) instance - i.e., change at the most permanent
pace layer.}

\end{document}

% Local Variables:
% TeX-engine: luatex
% End:
